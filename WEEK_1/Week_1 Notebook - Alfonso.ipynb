{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP - Week 1\n",
    "## Cleaning and preprocessing textual data\n",
    "\n",
    "#### In this weeks session we will cover some fundamentals to get started with natural language processing. This includes cleaning and preprocessing textual data in order to make further analysis easier and more efficient.\n",
    "\n",
    "###### We will cover:\n",
    "- Removal of unwanted characters\n",
    "- Normalisation\n",
    "- Tokenisation into n-grams\n",
    "- Stopword removal\n",
    "- Lemmatising/Stemming\n",
    "\n",
    "#### In order to give you as many different examples and show cases as possible I will not apply all this to one particular dataset but rather to individual textual examples, since text cleaning is task specific and there is no \"one-size-fits-all\" solution. In the practial you will apply suitable preprocessing techniques to a whole dataset of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "foxnews = pd.read_csv(\"foxnews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>succ</th>\n",
       "      <th>meta</th>\n",
       "      <th>user</th>\n",
       "      <th>mentions</th>\n",
       "      <th>prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>Merkel would never say NO</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>barryswallows</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>Expect more and more women to be asking .. \"wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>PostApocalypticHero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>Groping people in public wasn't already illega...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>californiamojo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>Merkel, possible the only person in charge who...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>MikeSte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>They know very well, no means NO! They need to...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  German lawmakers approve 'no means no' rape la...   \n",
       "1  German lawmakers approve 'no means no' rape la...   \n",
       "2  German lawmakers approve 'no means no' rape la...   \n",
       "3  German lawmakers approve 'no means no' rape la...   \n",
       "4  German lawmakers approve 'no means no' rape la...   \n",
       "\n",
       "                                                text  label succ  \\\n",
       "0                          Merkel would never say NO      1  NaN   \n",
       "1  Expect more and more women to be asking .. \"wh...      1  NaN   \n",
       "2  Groping people in public wasn't already illega...      0  NaN   \n",
       "3  Merkel, possible the only person in charge who...      1  NaN   \n",
       "4  They know very well, no means NO! They need to...      1  NaN   \n",
       "\n",
       "                                                meta                 user  \\\n",
       "0  German lawmakers passed a bill Thursday that w...        barryswallows   \n",
       "1  German lawmakers passed a bill Thursday that w...  PostApocalypticHero   \n",
       "2  German lawmakers passed a bill Thursday that w...       californiamojo   \n",
       "3  German lawmakers passed a bill Thursday that w...              MikeSte   \n",
       "4  German lawmakers passed a bill Thursday that w...            scientist   \n",
       "\n",
       "  mentions prev  \n",
       "0      NaN  NaN  \n",
       "1      NaN  NaN  \n",
       "2      NaN  NaN  \n",
       "3      NaN  NaN  \n",
       "4      NaN  NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foxnews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "Black Lives Matter must rescind anti-Israel declaration                                         64\n",
       "Confederate flag debate: Protecting hatred preserves freedom                                   156\n",
       "First lady Obama takes swipe at Trump                                                          161\n",
       "Fury as feminist blames toddler alligator death on white 'entitlement'                         165\n",
       "German lawmakers approve 'no means no' rape law after Cologne assaults                         121\n",
       "ICE program failing to rid US of dangerous illegal immigrants, analysts say                    162\n",
       "Navy names ship after gay rights advocate Harvey Milk                                          192\n",
       "States moving to restore work requirements for food stamp recipients                           141\n",
       "Supreme Court has spoken on affirmative action. Now, colleges should boost income diversity    114\n",
       "Texas, 12 states to ask judge to stall Obama transgender bathroom order                        244\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foxnews.groupby('title').text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the three types of text data in 3 separate dataframes\n",
    "texas = foxnews[foxnews['title'] == \"Texas, 12 states to ask judge to stall Obama transgender bathroom order\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Removing unwanted chatacters - which characters to remove?\n",
    "The is a primary step in the process of text cleaning. If we scrap some text from HTML/XML sources, we’ll need to get rid of all the tags, HTML entities, punctuation, non-alphabets, and any other kind of characters which might not be a part of the language. The general methods of such cleaning involve regular expressions, which can be used to filter out most of the unwanted texts.\n",
    "\n",
    "However, sometimes, depending on the type of data, we want to retain certain types of punctuation. Consider for example human generated tweets which you want to classify as very angry, angry, neutral, happy, and very happy. Simple sentiment analysis might find it hard to differentiate between a happy, and a very happy sentiment, because the only difference between a happy and a very happy tweet might be punctuation.\n",
    "\n",
    "Example:\n",
    "\n",
    "*This is amazing*    <pre>  vs     </pre>  *THIS IS AMAZING!!!!!*\n",
    "\n",
    "---\n",
    "\n",
    "Or what about this one\n",
    "\n",
    "\n",
    "\n",
    "*I don't know :) <3* <pre>  vs     </pre>  *I don't know :(((*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expression keeping only letters \n",
    "\n",
    "def keep_letters_only(raw_text):\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    return letters_only_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now we have reverse discrimination  BIG TIME because of this PERCEIVED PROBLEM that our DEAR LEADER intended to remedy by EXECUTIVE FIAT  Just another reason why there should not be a federal department of education that doles out OUR OWN TAX MONEY and makes the states JUMP THROUGH HOOPS TO GET IT  Don t you think ENOUGH IS ENOUGH '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Now we have reverse discrimination, BIG TIME because of this PERCEIVED PROBLEM that our DEAR LEADER intended to remedy by EXECUTIVE FIAT! Just another reason why there should not be a federal department of education that doles out OUR OWN TAX MONEY and makes the states JUMP THROUGH HOOPS TO GET IT! Don't you think ENOUGH IS ENOUGH!\"\n",
    "keep_letters_only(sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalisation\n",
    "Recall our sms sample:\n",
    "\n",
    "**** **** CONGRATlations **** You won 2 tIckETs to Hamilton in \n",
    "NYC http://www.hamiltonbroadway.com/J?NaIOl/event   wORtH over $500.00...CALL \n",
    "555-477-8914 or send message to: hamilton@freetix.com to get ticket !! !\n",
    "\n",
    "I'd definitely deem this as spam. But clearly there's a lot going on here: phone numbers, emails, website URLs, money amounts, and gratuitous whitespace and punctuation. Some terms are randomly capitalized, others are in all-caps. Since these terms might show up in any one of the training examples in countless forms, we need a way to ensure each training example is on equal footing via a preprocessing step called **normalisation**.\n",
    "\n",
    "Instead of removing the following terms, for each training example, let's replace them with a specific string.\n",
    "\n",
    "- Replace email addresses with `emailaddr`\n",
    "- Replace URLs with `httpaddr`\n",
    "- Replace money symbols with `moneysymb`\n",
    "- Replace phone numbers with `phonenumbr`\n",
    "- Replace numbers with `numbr`\n",
    "- get rid of all other punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation_sms(raw_text):\n",
    "    cleaned = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', raw_text)\n",
    "    cleaned = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n",
    "                     cleaned)\n",
    "    cleaned = re.sub(r'£|\\$|\\€', 'moneysymb ', cleaned) #add whitespace\n",
    "    cleaned = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr', cleaned)\n",
    "    cleaned = re.sub(r'\\d+(\\.\\d+)?', 'numbr', cleaned)\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", cleaned)\n",
    "    return letters_only_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now we have reverse discrimination  BIG TIME because of this PERCEIVED PROBLEM that our DEAR LEADER intended to remedy by EXECUTIVE FIAT  Just another reason why there should not be a federal department of education that doles out OUR OWN TAX MONEY and makes the states JUMP THROUGH HOOPS TO GET IT  Don t you think ENOUGH IS ENOUGH '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalisation_sms(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenisation\n",
    "Tokenisation is just the process of splitting a sentence into words.\n",
    "\n",
    "Text: *A bad day in London is still better than a bad day anywhere else*\n",
    "\n",
    "Tokens: `['a', 'bad', 'day', 'in', 'London', 'is', 'still', 'better', 'than', 'a', 'bad', 'day', 'anywhere', 'else']`\n",
    "\n",
    "This example not only divides the individual entities, but also gets rid of the capitalism involved (no pun intended). Capitalisation and De-capitalisation is again, dependent on the data and the task at hand. If we want to differentiate between any sentiments, then something written in uppercase might mean something different than something written in lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now',\n",
       " 'we',\n",
       " 'have',\n",
       " 'reverse',\n",
       " 'discrimination,',\n",
       " 'big',\n",
       " 'time',\n",
       " 'because',\n",
       " 'of',\n",
       " 'this',\n",
       " 'perceived',\n",
       " 'problem',\n",
       " 'that',\n",
       " 'our',\n",
       " 'dear',\n",
       " 'leader',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'remedy',\n",
       " 'by',\n",
       " 'executive',\n",
       " 'fiat!',\n",
       " 'just',\n",
       " 'another',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'there',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'federal',\n",
       " 'department',\n",
       " 'of',\n",
       " 'education',\n",
       " 'that',\n",
       " 'doles',\n",
       " 'out',\n",
       " 'our',\n",
       " 'own',\n",
       " 'tax',\n",
       " 'money',\n",
       " 'and',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'states',\n",
       " 'jump',\n",
       " 'through',\n",
       " 'hoops',\n",
       " 'to',\n",
       " 'get',\n",
       " 'it!',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'enough',\n",
       " 'is',\n",
       " 'enough!']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: If you have already removed all punctuations you can just use pythons inbuilt .split() function\n",
    "sample.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now',\n",
       " 'we',\n",
       " 'have',\n",
       " 'reverse',\n",
       " 'discrimination,',\n",
       " 'big',\n",
       " 'time',\n",
       " 'because',\n",
       " 'of',\n",
       " 'this',\n",
       " 'perceived',\n",
       " 'problem',\n",
       " 'that',\n",
       " 'our',\n",
       " 'dear',\n",
       " 'leader',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'remedy',\n",
       " 'by',\n",
       " 'executive',\n",
       " 'fiat!',\n",
       " 'just',\n",
       " 'another',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'there',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'federal',\n",
       " 'department',\n",
       " 'of',\n",
       " 'education',\n",
       " 'that',\n",
       " 'doles',\n",
       " 'out',\n",
       " 'our',\n",
       " 'own',\n",
       " 'tax',\n",
       " 'money',\n",
       " 'and',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'states',\n",
       " 'jump',\n",
       " 'through',\n",
       " 'hoops',\n",
       " 'to',\n",
       " 'get',\n",
       " 'it!',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'enough',\n",
       " 'is',\n",
       " 'enough!']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#However, if you still have punctuation, look what happens\n",
    "sample.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'London' and 'London,' are not the same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now',\n",
       " 'we',\n",
       " 'have',\n",
       " 'reverse',\n",
       " 'discrimination',\n",
       " ',',\n",
       " 'big',\n",
       " 'time',\n",
       " 'because',\n",
       " 'of',\n",
       " 'this',\n",
       " 'perceived',\n",
       " 'problem',\n",
       " 'that',\n",
       " 'our',\n",
       " 'dear',\n",
       " 'leader',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'remedy',\n",
       " 'by',\n",
       " 'executive',\n",
       " 'fiat',\n",
       " '!',\n",
       " 'just',\n",
       " 'another',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'there',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'federal',\n",
       " 'department',\n",
       " 'of',\n",
       " 'education',\n",
       " 'that',\n",
       " 'doles',\n",
       " 'out',\n",
       " 'our',\n",
       " 'own',\n",
       " 'tax',\n",
       " 'money',\n",
       " 'and',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'states',\n",
       " 'jump',\n",
       " 'through',\n",
       " 'hoops',\n",
       " 'to',\n",
       " 'get',\n",
       " 'it',\n",
       " '!',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'enough',\n",
       " 'is',\n",
       " 'enough',\n",
       " '!']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so it's better to use nltk's tokenize\n",
    "from nltk import word_tokenize, bigrams, trigrams\n",
    "\n",
    "nltk_tokens = word_tokenize(sample.lower())\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('now', 'we'), ('we', 'have'), ('have', 'reverse'), ('reverse', 'discrimination'), ('discrimination', ','), (',', 'big'), ('big', 'time'), ('time', 'because'), ('because', 'of'), ('of', 'this'), ('this', 'perceived'), ('perceived', 'problem'), ('problem', 'that'), ('that', 'our'), ('our', 'dear'), ('dear', 'leader'), ('leader', 'intended'), ('intended', 'to'), ('to', 'remedy'), ('remedy', 'by'), ('by', 'executive'), ('executive', 'fiat'), ('fiat', '!'), ('!', 'just'), ('just', 'another'), ('another', 'reason'), ('reason', 'why'), ('why', 'there'), ('there', 'should'), ('should', 'not'), ('not', 'be'), ('be', 'a'), ('a', 'federal'), ('federal', 'department'), ('department', 'of'), ('of', 'education'), ('education', 'that'), ('that', 'doles'), ('doles', 'out'), ('out', 'our'), ('our', 'own'), ('own', 'tax'), ('tax', 'money'), ('money', 'and'), ('and', 'makes'), ('makes', 'the'), ('the', 'states'), ('states', 'jump'), ('jump', 'through'), ('through', 'hoops'), ('hoops', 'to'), ('to', 'get'), ('get', 'it'), ('it', '!'), ('!', 'do'), ('do', \"n't\"), (\"n't\", 'you'), ('you', 'think'), ('think', 'enough'), ('enough', 'is'), ('is', 'enough'), ('enough', '!')]\n",
      "*****---------------------------------------------------------*****\n",
      "[('now', 'we', 'have'), ('we', 'have', 'reverse'), ('have', 'reverse', 'discrimination'), ('reverse', 'discrimination', ','), ('discrimination', ',', 'big'), (',', 'big', 'time'), ('big', 'time', 'because'), ('time', 'because', 'of'), ('because', 'of', 'this'), ('of', 'this', 'perceived'), ('this', 'perceived', 'problem'), ('perceived', 'problem', 'that'), ('problem', 'that', 'our'), ('that', 'our', 'dear'), ('our', 'dear', 'leader'), ('dear', 'leader', 'intended'), ('leader', 'intended', 'to'), ('intended', 'to', 'remedy'), ('to', 'remedy', 'by'), ('remedy', 'by', 'executive'), ('by', 'executive', 'fiat'), ('executive', 'fiat', '!'), ('fiat', '!', 'just'), ('!', 'just', 'another'), ('just', 'another', 'reason'), ('another', 'reason', 'why'), ('reason', 'why', 'there'), ('why', 'there', 'should'), ('there', 'should', 'not'), ('should', 'not', 'be'), ('not', 'be', 'a'), ('be', 'a', 'federal'), ('a', 'federal', 'department'), ('federal', 'department', 'of'), ('department', 'of', 'education'), ('of', 'education', 'that'), ('education', 'that', 'doles'), ('that', 'doles', 'out'), ('doles', 'out', 'our'), ('out', 'our', 'own'), ('our', 'own', 'tax'), ('own', 'tax', 'money'), ('tax', 'money', 'and'), ('money', 'and', 'makes'), ('and', 'makes', 'the'), ('makes', 'the', 'states'), ('the', 'states', 'jump'), ('states', 'jump', 'through'), ('jump', 'through', 'hoops'), ('through', 'hoops', 'to'), ('hoops', 'to', 'get'), ('to', 'get', 'it'), ('get', 'it', '!'), ('it', '!', 'do'), ('!', 'do', \"n't\"), ('do', \"n't\", 'you'), (\"n't\", 'you', 'think'), ('you', 'think', 'enough'), ('think', 'enough', 'is'), ('enough', 'is', 'enough'), ('is', 'enough', '!')]\n"
     ]
    }
   ],
   "source": [
    "print(list(bigrams(nltk_tokens)))\n",
    "print(\"*****---------------------------------------------------------*****\")\n",
    "print(list(trigrams(nltk_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stopword Removal\n",
    "Stopwords are the words which are used very frequently. Words like “of, are, the, it, is” are some examples of stopwords. In applications like document search engines and document classification, where keywords are more important than general terms, removing stopwords can be a good idea. However, if there’s some application about, for instance, songs lyrics search, or searching for specific quotes, stopwords can be important. \n",
    "\n",
    "“To be, or not not be” - Stopwords in such phrases actually play an important role, and hence, should not be dropped.\n",
    "\n",
    "Another example is negation. \"not\" is contained in many stopword lists, but deleting \"not\" out of a negative review can make a positive out of it.\n",
    "\n",
    "There are two common approaches of removing the stopwords, and both are fairly straightforward. One way is to count all the word occurrences, and providing a threshold value on the count, and getting rid of all the terms/words occurring more than the specified threshold value. The other way is to have a predetermined list of stopwords, which can be removed from the list of tokens/tokenised sentences. I personally, believe the second one is better, as determining thresholds can be quite difficult and you can use tf-idf (more on that next lesson) to weigh the importance of words.\n",
    "\n",
    "NLTK comes with many corpora, including a stopword list. This list contains around 200 terms. For my research, however, I use one that contains over 600 terms: http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop (I removed apostrophes as I remove punctuations before I remove stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = 'SmartStoplist.txt'\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.extend(line.split())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK alternative\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "filtered_sentence_nltk = [w for w in nltk_tokens if not w in stop_words_nltk]\n",
    "filetered_sentence_smart = [w for w in nltk_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lemmatising and Stemming\n",
    "Lemmatisation and stemming both refer to a process of reducing a word to its root. The difference is that stem might not be an actual word whereas, a lemma is an actual word. It’s a handy tool if you want to avoid treating different forms of the same word as different words, e.g. *love, loved, loving*\n",
    "\n",
    "**Lemmatising:** considered, considers, consider → “consider”\n",
    "\n",
    "**Stemming:** considered, considering, consider → “consid”\n",
    "\n",
    "I personally have never noticed a significat difference between lemmatising and stemming when training classifiers. However, I suggest you try out yourself. NLTK comes with many different in-built lemmatisers and stemmers, so just plug and play.\n",
    "\n",
    "A note of caution: WordNetLemmatizer requires a POS-tag. The default is set to \"noun\" and therefore doesn't work with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consid\n",
      "considers\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = \"considers\"\n",
    "word_2 = \"apple\"\n",
    "\n",
    "stemmed_word =  stemmer.stem(word)\n",
    "lemmatised_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "stemmed_word_2 =  stemmer.stem(word_2)\n",
    "lemmatised_word_2 = lemmatizer.lemmatize(word_2)\n",
    "\n",
    "print(stemmed_word)\n",
    "print(lemmatised_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together - in one preprocess() function\n",
    "Now that we covered everything we need to know, we can combine everything into one function and apply it to the whole data. Let's keep it simple and write one for the news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news(raw_text):\n",
    "    \n",
    "    #keeping only letters\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and tokenise\n",
    "    tokens = word_tokenize(letters_only_text.lower())\n",
    "    \n",
    "\n",
    "    cleaned_words = []\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # remove stopwords\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    # stemm or lemmatise words\n",
    "    stemmed_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = stemmer.stem(word)\n",
    "        stemmed_words.append(word)\n",
    "    \n",
    "    # converting list back to string\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "foxnews['prep_text'] = foxnews['text'].apply(preprocess_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>succ</th>\n",
       "      <th>meta</th>\n",
       "      <th>user</th>\n",
       "      <th>mentions</th>\n",
       "      <th>prev</th>\n",
       "      <th>prep_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>Merkel would never say NO</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>barryswallows</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>merkel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>Expect more and more women to be asking .. \"wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>PostApocalypticHero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>expect women men longer interest touch pull pant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>Groping people in public wasn't already illega...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>californiamojo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>grope peopl public wasn illeg deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>Merkel, possible the only person in charge who...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>MikeSte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>merkel person charg wors obama hardest merkel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>German lawmakers approve 'no means no' rape la...</td>\n",
       "      <td>They know very well, no means NO! They need to...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>German lawmakers passed a bill Thursday that w...</td>\n",
       "      <td>scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mean pass law make legal castrat anim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>Fury as feminist blames toddler alligator deat...</td>\n",
       "      <td>No, 10000 Loose nuts off their meds!</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A white Chicago-based Social Justice Warrior w...</td>\n",
       "      <td>toorotten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loos nut med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>Fury as feminist blames toddler alligator deat...</td>\n",
       "      <td>Just another ugly leftist.</td>\n",
       "      <td>1</td>\n",
       "      <td>True. Most leftists ,esp female leftists have ...</td>\n",
       "      <td>A white Chicago-based Social Justice Warrior w...</td>\n",
       "      <td>randwolf13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ugli leftist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>Fury as feminist blames toddler alligator deat...</td>\n",
       "      <td>True. Most leftists ,esp female leftists have ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A white Chicago-based Social Justice Warrior w...</td>\n",
       "      <td>creatingfalserealitiesleadstoinsanity</td>\n",
       "      <td>@randwolf13</td>\n",
       "      <td>Just another ugly leftist.</td>\n",
       "      <td>true leftist esp femal leftist violent hatr pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>Fury as feminist blames toddler alligator deat...</td>\n",
       "      <td>First, lets get this straight: a white, gay ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A white Chicago-based Social Justice Warrior w...</td>\n",
       "      <td>KhaosDominus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straight white gay male pretend feminist brien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>Fury as feminist blames toddler alligator deat...</td>\n",
       "      <td>White privilege ...work all your life to take ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A white Chicago-based Social Justice Warrior w...</td>\n",
       "      <td>leftisrighthippy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white privileg work life care peopl work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     German lawmakers approve 'no means no' rape la...   \n",
       "1     German lawmakers approve 'no means no' rape la...   \n",
       "2     German lawmakers approve 'no means no' rape la...   \n",
       "3     German lawmakers approve 'no means no' rape la...   \n",
       "4     German lawmakers approve 'no means no' rape la...   \n",
       "...                                                 ...   \n",
       "1523  Fury as feminist blames toddler alligator deat...   \n",
       "1524  Fury as feminist blames toddler alligator deat...   \n",
       "1525  Fury as feminist blames toddler alligator deat...   \n",
       "1526  Fury as feminist blames toddler alligator deat...   \n",
       "1527  Fury as feminist blames toddler alligator deat...   \n",
       "\n",
       "                                                   text  label  \\\n",
       "0                             Merkel would never say NO      1   \n",
       "1     Expect more and more women to be asking .. \"wh...      1   \n",
       "2     Groping people in public wasn't already illega...      0   \n",
       "3     Merkel, possible the only person in charge who...      1   \n",
       "4     They know very well, no means NO! They need to...      1   \n",
       "...                                                 ...    ...   \n",
       "1523               No, 10000 Loose nuts off their meds!      1   \n",
       "1524                         Just another ugly leftist.      1   \n",
       "1525  True. Most leftists ,esp female leftists have ...      1   \n",
       "1526  First, lets get this straight: a white, gay ma...      1   \n",
       "1527  White privilege ...work all your life to take ...      1   \n",
       "\n",
       "                                                   succ  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1523                                                NaN   \n",
       "1524  True. Most leftists ,esp female leftists have ...   \n",
       "1525                                                NaN   \n",
       "1526                                                NaN   \n",
       "1527                                                NaN   \n",
       "\n",
       "                                                   meta  \\\n",
       "0     German lawmakers passed a bill Thursday that w...   \n",
       "1     German lawmakers passed a bill Thursday that w...   \n",
       "2     German lawmakers passed a bill Thursday that w...   \n",
       "3     German lawmakers passed a bill Thursday that w...   \n",
       "4     German lawmakers passed a bill Thursday that w...   \n",
       "...                                                 ...   \n",
       "1523  A white Chicago-based Social Justice Warrior w...   \n",
       "1524  A white Chicago-based Social Justice Warrior w...   \n",
       "1525  A white Chicago-based Social Justice Warrior w...   \n",
       "1526  A white Chicago-based Social Justice Warrior w...   \n",
       "1527  A white Chicago-based Social Justice Warrior w...   \n",
       "\n",
       "                                       user     mentions  \\\n",
       "0                             barryswallows          NaN   \n",
       "1                       PostApocalypticHero          NaN   \n",
       "2                            californiamojo          NaN   \n",
       "3                                   MikeSte          NaN   \n",
       "4                                 scientist          NaN   \n",
       "...                                     ...          ...   \n",
       "1523                              toorotten          NaN   \n",
       "1524                             randwolf13          NaN   \n",
       "1525  creatingfalserealitiesleadstoinsanity  @randwolf13   \n",
       "1526                           KhaosDominus          NaN   \n",
       "1527                       leftisrighthippy          NaN   \n",
       "\n",
       "                            prev  \\\n",
       "0                            NaN   \n",
       "1                            NaN   \n",
       "2                            NaN   \n",
       "3                            NaN   \n",
       "4                            NaN   \n",
       "...                          ...   \n",
       "1523                         NaN   \n",
       "1524                         NaN   \n",
       "1525  Just another ugly leftist.   \n",
       "1526                         NaN   \n",
       "1527                         NaN   \n",
       "\n",
       "                                              prep_text  \n",
       "0                                                merkel  \n",
       "1      expect women men longer interest touch pull pant  \n",
       "2             grope peopl public wasn illeg deutschland  \n",
       "3     merkel person charg wors obama hardest merkel ...  \n",
       "4                 mean pass law make legal castrat anim  \n",
       "...                                                 ...  \n",
       "1523                                       loos nut med  \n",
       "1524                                       ugli leftist  \n",
       "1525  true leftist esp femal leftist violent hatr pr...  \n",
       "1526  straight white gay male pretend feminist brien...  \n",
       "1527           white privileg work life care peopl work  \n",
       "\n",
       "[1528 rows x 9 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foxnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
